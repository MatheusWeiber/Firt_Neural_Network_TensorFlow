# Firt_Neural_Network_TensorFlow

Projeto: Classifica√ß√£o de Cobertura do Solo com Redes Neurais (TensorFlow/Keras)

üáßüá∑ Vers√£o em Portugu√™s

Vis√£o Geral

Este projeto √© minha primeira implementa√ß√£o de uma rede neural profunda (Deep Learning) utilizando as bibliotecas TensorFlow e Keras. O objetivo foi construir, treinar e avaliar um modelo de Multi-Layer Perceptron (MLP) capaz de resolver um problema complexo de classifica√ß√£o multiclasse: o dataset Covertype da UCI.

O modelo alcan√ßou uma acur√°cia de ~88% no conjunto de teste, demonstrando a capacidade do Keras em modelar rela√ß√µes n√£o-lineares complexas.

O Desafio: Dataset Covertype

Problema: Classifica√ß√£o (Supervisionada).

Dataset: Cont√©m 581.012 amostras de √°reas florestais de 30x30m.

Features (Entrada): 54 colunas, incluindo dados quantitativos (Eleva√ß√£o, Inclina√ß√£o, Dist√¢ncias) e dados bin√°rios (Tipos de Solo, √Åreas Selvagens).

Alvo (Sa√≠da): 7 classes √∫nicas (tipos de cobertura florestal, ex: "Spruce-Fir", "Lodgepole Pine").

Metodologia e Arquitetura

O pipeline do projeto seguiu as melhores pr√°ticas de Deep Learning:

Pr√©-processamento (sklearn):

Os dados foram divididos em conjuntos de treino (80%) e teste (20%).

As 10 features quantitativas foram normalizadas usando StandardScaler para otimizar a converg√™ncia da rede neural.

As 44 features bin√°rias foram mantidas intactas.

Ajuste do Alvo: As 7 classes (originais 1-7) foram ajustadas para o formato esperado pelo Keras (0-6).

Constru√ß√£o do Modelo (keras.Sequential):

Camada de Entrada: Dense(100, activation='relu', input_shape=(54,))

Camada Oculta: Dense(50, activation='relu')

Regulariza√ß√£o: Dropout(0.2) para prevenir overfitting (decoreba).

Camada de Sa√≠da: Dense(7, activation='softmax') para produzir as probabilidades das 7 classes.

Compila√ß√£o e Treinamento:

Otimizador: adam

Fun√ß√£o de Perda: sparse_categorical_crossentropy (ideal para alvos inteiros multiclasse).

Treino: O modelo foi treinado por 50 √©pocas, com batch_size=64, utilizando o conjunto de teste como validation_data para monitorar o desempenho.

Resultados

Acur√°cia Final: ~88%

An√°lise: O modelo demonstrou excelente capacidade de generaliza√ß√£o. Os gr√°ficos de hist√≥rico de treino (acur√°cia/loss vs. val_acur√°cia/val_loss) mostraram que o Dropout foi eficaz em prevenir overfitting. A matriz de confus√£o revelou que, embora o modelo seja forte, sua principal dificuldade √© distinguir as Classes 1 e 2, que s√£o geologicamente muito similares.

üá∫üá∏ English Version

Project: Forest Cover Type Classification with Neural Networks (TensorFlow/Keras)

Overview

This project is my first implementation of a deep neural network using the TensorFlow and Keras libraries. The objective was to build, train, and evaluate a Multi-Layer Perceptron (MLP) model capable of solving a complex, multi-class classification problem: the UCI Covertype dataset.

The model achieved an accuracy of ~88% on the test set, demonstrating Keras's capability in modeling complex non-linear relationships.

The Challenge: Covertype Dataset

Problem: Classification (Supervised).

Dataset: Contains 581,012 samples of 30x30m forest areas.

Features (Input): 54 columns, including quantitative data (Elevation, Slope, Distances) and binary data (Soil Types, Wilderness Areas).

Target (Output): 7 unique classes (types of forest cover, e.g., "Spruce-Fir", "Lodgepole Pine").

Methodology and Architecture

The project pipeline followed deep learning best practices:

Preprocessing (sklearn):

Data was split into training (80%) and testing (20%) sets.

The 10 quantitative features were normalized using StandardScaler to optimize the neural network's convergence.

The 44 binary features were passed through unchanged.

Target Adjustment: The 7 classes (originally 1-7) were adjusted to the Keras-expected format (0-6).

Model Building (keras.Sequential):

Input Layer: Dense(100, activation='relu', input_shape=(54,))

Hidden Layer: Dense(50, activation='relu')

Regularization: Dropout(0.2) to prevent overfitting.

Output Layer: Dense(7, activation='softmax') to produce the probability distribution for the 7 classes.

Compilation and Training:

Optimizer: adam

Loss Function: sparse_categorical_crossentropy (ideal for multi-class integer targets).

Training: The model was trained for 50 epochs with a batch_size=64, using the test set as validation_data to monitor performance.

Results

Final Accuracy: ~88%

Analysis: The model demonstrated excellent generalization capabilities. The training history plots (accuracy/loss vs. val_accuracy/val_loss) showed that Dropout was effective in preventing overfitting. The confusion matrix revealed that while the model is strong, its primary difficulty is in distinguishing between Classes 1 and 2, which are geologically very similar.
